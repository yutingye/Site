<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="content-type" content="text/html"/>
<meta name="description" content="Research work of Dr. Yuting Ye in computer graphics"/>
<meta name="keywords" content="Animation"/> 
<meta name="author" content="Yuting Ye"/> 
<link rel="stylesheet" type="text/css" href="default.css"/>
<title>Research</title>
</head>

<body>

<div class="main">

    <div class="container">

        <div class="gfx"><a><span></span></a></div>

        <div class="menu">
            <a href="index.html"><span>Home</span></a>
            <a href="Research.html"><span>Research</span></a>
            <a href="resume.html"><span>Resume</span></a>       
            <a href="yuting-cv.pdf"><span>CV (pdf)</span></a>
            <a id="last"><span> </span></a>
        </div>

        <div class="content">
            
            <div style="border: none; height: 0px; position: relative; top: -200px; left: 80px;">
                <div style="font: normal 20pt serif,'Times New Roman'; color: #777">Yuting Ye</div>
                <div style="font: normal 12pt serif,'Times New Roman'; color: #888; line-height: 1.5em">
                    Ph.D.<br/>
                    Researcher<br/>
                    yutingye.public AT gmail.com<br/>
                </div>
            </div>
              
            <div class="item">
                <!-- p style="height: 120px;">
				<a target="_blank" href="">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 128px; width: 175px; left: -10px; top: 10px; z-index: 1;"/>
				<img src="Research_files/teaser_.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -180px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 128px; width: 175px; left: -250px; top: -10px;"/>
				</a>
				<div class="text">
					Authors. Year. <b>Title</b> <i>Venue</i>
					<br/>[
					<a target="_blank" href="">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="">Paper</a>&nbsp;&nbsp;&nbsp;
					<a href="">Code</a>
					]<br style="line-height: 2.0em"/>
					Brief description.
				</div>
				</p -->

                <!--p style="height: 120px;">
				<a target="_blank" href="">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 128px; width: 175px; left: -10px; top: 10px; z-index: 1;"/>
				<img src="Research_files/teaser_TOG23.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -180px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 128px; width: 175px; left: -250px; top: -10px;"/>
				</a>
				<div class="text">
					Alex Adkins, Lorraine Lin, Aline Normoyle, Yun Sun, Max Di Luca, Yuting Ye, Sophie JÃ¶rg. 2023. <b>How important are Hand Motions in Communication Through the Lense of Charade</b> <i>ACM Trans. Graph. (TOG)</i>
					<br/>[
					<a target="_blank" href="">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="">Paper</a>&nbsp;&nbsp;&nbsp;
					<a href="">Code</a>
					]<br style="line-height: 2.0em"/>
					Brief description.
				</div>
				</p -->

                <!--p style="height: 100px;">
				<a target="_blank" href="">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 128px; width: 175px; left: -10px; top: 10px; z-index: 1;"/>
				<img src="Research_files/teaser_EG23.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -180px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 128px; width: 175px; left: -250px; top: -10px;"/>
				</a>
				<div class="text">
					Yunbo Zhang, Alex Clegg, Sehoon Ha, Greg Turk, Yuting Ye. 2023. <b>Learning to Transfer In-Hand Manipulations Using a Greedy Shape Curriculum.</b> <i>Eurographics</i>
					<br/>[
					<a target="_blank" href="">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="">Paper</a>&nbsp;&nbsp;&nbsp;
					<a href="">Code</a>
					]<br style="line-height: 2.0em"/>
					Brief description.
				</div>
				</p -->

                <p style="height: 120px;">
				<a target="_blank" href="https://openaccess.thecvf.com/content/WACV2023/html/Feng_Rethinking_the_Data_Annotation_Process_for_Multi-View_3D_Pose_Estimation_WACV_2023_paper.html">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 133px; width: 175px; left: -10px; top: 8px; z-index: 1;"/>
				<img src="Research_files/teaser_WACV23.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -181px; top: 0px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 133px; width: 175px; left: -353px; top: 8px;"/>
				</a>
				<div class="text">
					Qi Feng, Kun He, He Wen, Cem Keskin, Yuting Ye. 2023. <b>Rethinking the Data Annotation Process for Multi-view 3D Pose Estimation with Active Learning and Self-Training.</b> <i>WACV</i>
					<br/>[
					<a target="_blank" href="https://openaccess.thecvf.com/content/WACV2023/html/Feng_Rethinking_the_Data_Annotation_Process_for_Multi-View_3D_Pose_Estimation_WACV_2023_paper.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://openaccess.thecvf.com/content/WACV2023/papers/Feng_Rethinking_the_Data_Annotation_Process_for_Multi-View_3D_Pose_Estimation_WACV_2023_paper.pdf">Paper</a>&nbsp;&nbsp;
					]<br style="line-height: 2.0em"/>
					We devised an active learning strategy to reduce manual annotation effort for 3D pose estimation problems using multi-view consistency and self-training with pseudo labels.
				</div>
				</p>

                <p style="height: 100px;">
				<a target="_blank" href="https://research.facebook.com/publications/questsim-human-motion-tracking-from-sparse-sensors-with-simulated-avatars/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 178px; left: -10px; top: -12px; z-index: 1;"/>
				<img src="Research_files/teaser_SIGA22_QS.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -22px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 178px; left: -356px; top: -12px;"/>
				</a>
				<div class="text">
					Alexander W. Winkler, Jungdam Won, Yuting Ye. 2022. <b>QuestSim: Human Motion Tracking From Sparse Sensors With Simulated Avatars.</b> <i>SIGGRAPH Asia Conference</i>
					<br/>[
					<a target="_blank" href="https://research.facebook.com/publications/questsim-human-motion-tracking-from-sparse-sensors-with-simulated-avatars/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://research.facebook.com/file/500550158623023/QuestSim--Human-Motion-Tracking-from-Sparse-Sensors-with-Simulated-Avatars.pdf">Paper
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>&nbsp;&nbsp;
					<a href="https://youtu.be/CkTHsz6Ldas">Video 
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We apply deep reinforcement learning to control a simulated full body avatar in real time. The avatar is driven only by input signals from a VR headset and two controllers, and it is able to reliably follow the user's motion in a variety of activities.
				</div>
				</p>

                <p style="height: 100px;">
				<a target="_blank" href="https://tml.stanford.edu/publications/2022/transformer-inertial-poser-real-time-human-motion-reconstruction-sparse-imus">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 178px; left: -11px; top: -15px; z-index: 1;"/>
				<img src="Research_files/teaser_SIGA22_TIP.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -25px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 178px; left: -357px; top: -15px;"/>
				</a>
				<div class="text">
					Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, C. Karen Liu. 2022. <b>Transformer Inertial Poser: Real-time Human Motion Reconstruction From Sparse IMUs With Simultaneous Terrain Generation.</b> <i>SIGGRAPH Asia Conference</i>
					<br/>[
					<a target="_blank" href="https://tml.stanford.edu/publications/2022/transformer-inertial-poser-real-time-human-motion-reconstruction-sparse-imus">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://arxiv.org/pdf/2203.15720.pdf">Paper  
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/rXb6SaXsnc0">Video 
                    <img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://github.com/jyf588/transformer-inertial-poser">Code 
					<img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We reconstruct both the human motion and the environment from 6 body-worn IMUs.
				</div>
				</p>

				<p style="height: 100px;">
				<a target="_blank" href="https://research.fb.com/publications/evaluating-grasping-visualizations-and-control-modes-in-a-vr-game/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 178px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_TAP21.jpg" style="position: relative; height: 120px; width: 160px; left: -183px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 178px; left: -356px; top: -10px;"></img></a>
				<div class="text">
					 Alex Adkins, Lorraine Lin, Aline Normoyle, Ryan Canales, Yuting Ye, Sophie JÃ¶rg. 2021. <b>Evaluating Grasping Visualizations and Control Modes in a VR Game.</b> <i>ACM Transactions on Applied Perception (TAP) </i>
					<br/>[
					<a target="_blank" href="https://research.fb.com/publications/evaluating-grasping-visualizations-and-control-modes-in-a-vr-game/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://dl.acm.org/doi/10.1145/3486582?cid=81335499985">Paper&nbsp;
					<img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;
					<a href="https://www.facebook.com/watch/?v=220461743440901">Video</a>
					]<br style="line-height: 2.0em"/>
					We built a VR Escape Room game to study virtual hand-object manipulation. Immersive virtual environment is an important factor in VR perception studies.
				</div>
				</p>

				<p style="height: 100px;">
				<a target="_blank" href="https://github.com/cghezhang/ManipNet">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 178px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG21.jpg" style="position: relative; height: 120px; width: 160px; left: -183px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 178px; left: -356px; top: -10px;"></img></a>
				<div class="text">
					He Zhang, Yuting Ye, Takaaki Shiratori, Taku Komura. 2021. <b>ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
					<br/>[
					<a target="_blank" href="https://github.com/cghezhang/ManipNet">Project with Code
					<img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="http://www.ipab.inf.ed.ac.uk/cgvu/zhang2021.pdf">Paper&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3450626.3459830?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/xvAq8ax5cuE">Video
					<img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>&nbsp;
					]<br style="line-height: 2.0em"/>
					We synthesize detailed finger motions for in-hand object manipulation from trajctories of the wrists and the objects, by representing their geometric relationship.
				</div>
				</p>

				<p style="height: 100px;">
				<a target="_blank" href="https://arxiv.org/abs/2109.15299">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 137px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_diform.jpg" style="position: relative; height: 122px; width: 160px; left: -182px; top: -19px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 137px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
					Binbin Xu, Lingni Ma, Yuting Ye, Tanner Schmidt, Christopher D. Twigg, Steven Lovegrove. 2021. <b>Identity-Disentangled Neural Deformation Model for Dynamic Meshes.</b> <i>Arxiv: 2109.15299</i>
					<br/>[
					<a href="https://arxiv.org/pdf/2109.15299.pdf">Paper&nbsp;
					<img src="Research_files/arxiv.png" width="20" height="20" style="vertical-align:top"/></a>&nbsp;
					]<br style="line-height: 2.0em"/>
					We learn a Deep-SDF like neural deformation model with disentangled identity and shape latent space from unregistered and partial 4D scans for pose and shape estimation.
				</div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="https://zhouyisjtu.github.io/project_vcmeshcnn/vcmeshcnn.html">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 128px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/highres_body.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -15px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 128px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Yi Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li, Yaser Sheikh. 2020. <b>Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying Kernels.</b> <i>NeurIPS</i>
                    <br/>[ 
                    <a target="_blank" href="https://zhouyisjtu.github.io/project_vcmeshcnn/vcmeshcnn.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://arxiv.org/pdf/2006.04325.pdf">Paper</a> &nbsp; &nbsp;
                    <a href="https://github.com/facebookresearch/VCMeshConv">Code</a>
                    <a href="https://github.com/facebookresearch/VCMeshConv"><img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a> 
                    ]<br style="line-height: 2.0em"/>
					Mesh convolution is challenging due to irregular local connectivities. We proposed a novel convolution operator with globally shared weights and varying local coefficients.
				</div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="https://dl.acm.org/doi/10.1145/3415263.3419155">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/HandsCourse.jpg" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img></a>
				<div class="text">
                    Sophie JÃ¶rg, Yuting Ye, Michael Neff, Franziska Mueller, Victor Zordan. 2020. <b>Virtual hands in VR: motion capture, synthesis, and perception.</b> <i>SIGGRAPH and SIGGRAPH Asia Courses</i>.&nbsp; &nbsp; [ 
					<a target="_blank" href="https://www.dropbox.com/sh/3rnptvg27aifmda/AADD-7gIj6kri4ZjOunav3mqa">Course Notes</a> &nbsp; &nbsp;
					<a target="_blank" href="https://dl.acm.org/doi/10.1145/3415263.3419155?cid=81335499985">Presentation
					<img src="Research_files/dlauthorizer.jpg" width="20" height="20" alt="ACM Digital Library" style="vertical-align:top"/></a> ]<br/>
					In this course, we cover state-of-the-art methods of hand representation and animation in virtual reality. Topics include finger motion capture hardware and algorithms, hand animation with or without physics, and hand perceptions in VR.
				</div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="https://research.fb.com/publications/megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG20.jpg" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img></a>
				<div class="text">
                    Shangchen Han, Beibei Liu, Randi Cabezas, Christopher D Twigg, Peizhao Zhang, Jeff Petkau, Tsz-Ho Yu, Chun-Jung Tai, Muzaffer Akbay, Zheng Wang, Asaf Nitzan, Gang Dong, Yuting Ye, Lingling Tao, Chengde Wan, Robert Wang. 2020. <b>MEgATrack: monochrome egocentric articulated hand-tracking for virtual reality.</b> <i>SIGGRAPH</i>
                    <br/>[ 
                    <a target="_blank" href="https://research.fb.com/publications/megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://dl.acm.org/doi/10.1145/3386569.3392452?cid=81335499985">Paper&nbsp;
					<img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a> &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.facebook.com/watch/?v=227793185857989">Video</a>
                    ]<br style="line-height: 2.0em"/>
					Hand tracking from monochrome cameras on the standalone Quest VR headsets.
				</div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="https://www.bmvc2020-conference.com/conference/papers/paper_0481.html">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 130px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/Global.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -15px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 130px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Julian Habekost, Takaaki Shiratori, Yuting Ye and Taku Komura. 2020. <b>Learning 3D Global Human Motion Estimation from Unpaired, Disjoint Datasets.</b> <i>BMVC</i>
                    <br/>[ 
                    <a target="_blank" href="https://www.bmvc2020-conference.com/conference/papers/paper_0481.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.bmvc2020-conference.com/assets/papers/0481.pdf">Paper</a> &nbsp; &nbsp;
                    <a href="https://youtu.be/ZuE78scOVOQ">Video</a>
                    <a href="https://youtu.be/ZuE78scOVOQ"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a> 
                    ]<br style="line-height: 2.0em"/>
                	Natural human motions are constrained to maintain contacts and body size over time. These are useful information to estimate global motions from monocular videos. We show how to learn such prior from a generic motion database independent of the videos.  
				</div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="https://arxiv.org/abs/1904.07528">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 135px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/Disentangle.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -18px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 135px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Yikang Li, Chris Twigg, Yuting Ye, Lingling Tao, Xiaogang Wang. 2019. <b>Disentangling Pose from Appearance in Monochrome Hand Images.</b> <i>ICCV <a target="_blank" href="https://sites.google.com/view/hands2019/home">Hands</a> Workshop</i>
                    <br/>[ 
                    <a href="https://arxiv.org/pdf/1904.07528.pdf">Paper&nbsp;
					<img src="Research_files/arxiv.png" width="20" height="20" style="vertical-align:top"/></a>&nbsp;
                    ]<br style="line-height: 2.0em"/>
                	We explored self-supervised learning to disentangle the hand pose from its appearance in an monochrome image from unpaired data. We show that the disentanglement improves pose estimation robustness at the presence of appearance variations.
				</div>
				</p>
            
                <p style="height: 100px;">
                <a target="_blank" href="https://rcanale827d.myportfolio.com/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 130px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/Grasp.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -18px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 130px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Ryan Canales, Aline Normoyle, Yu Sun, Yuting Ye, Massimiliano Di Luca, Sophie JÃ¶rg. 2019. <b>Virtual Grasping Feedback and Virtual Hand Ownership.</b> <i>ACM SAP</i>
                    <br/>[ 
                    <a target="_blank" href="https://rcanale827d.myportfolio.com/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://docs.wixstatic.com/ugd/fd7532_889bdfdc831b4ca7b98ee4878adb3afd.pdf">Paper&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3343036.3343132?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://youtu.be/hI8joIgPwUM">Video</a>
                    <a href="https://youtu.be/hI8joIgPwUM"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a> 
                    ]<br style="line-height: 2.0em"/>
                	We experimented with different visual styles for grasping virtual objects in VR. We found that showing the real hand poses with object penetration helps grasping efficiency, but users prefer physically consistent penetration-free poses.
				</div>
				</p>

                <p style="height: 100px;">
                <a target="_blank" href="https://www.lorrainelin.com/vrpuzzlegame">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/toyblocks_burger.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img></a>
				<div class="text">
                    Lorraine Lin, Aline Normoyle, Alexandra Adkins, Yun Sun, Andrew Robb, Yuting Ye, Max Di Luca, Sophie JÃ¶rg. 2019. <b>The Effect of Hand Size and Interaction Modality on the Virtual Hand Illusion.</b> <i>IEEE VR</i>
                    <br/>[ 
                    <a target="_blank" href="https://www.lorrainelin.com/vrpuzzlegame">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.lorrainelin.com/s/2019_Lin_et_al_Hand_Size_and_Interaction_Modality.pdf">Paper&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://ieeexplore.ieee.org/document/8797787"><img src="Research_files/logo-ieee.png" width="32" height="22" alt="IEEE Explorer" style="vertical-align:middle"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://youtu.be/soPGP7AiC9Q">Video</a>
                    <a href="https://youtu.be/soPGP7AiC9Q"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a> 
                    ]<br style="line-height: 2.0em"/>
                    Our VR experiment found that a matching hand size with dextrous finger tracking are more preferrable for most users, but other conditions still provide a fun experience.
                </div>
				</p>
    
                <p style="height: 100px;">
                <a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6416793/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 130px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/SNN.jpg" style="position: relative; height: 120px; width: 160px; left: -182px; top: -17px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 130px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, Kaushik Roy. 2019. <b>Going Deeper in Spiking Neural Networks: VGG and Residual Architectures.</b> <i>Frontiers in Neuroscience</i>
                    <br/>[ 
                    <a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6416793/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6416793/pdf/fnins-13-00095.pdf">Paper</a>
                    ]<br style="line-height: 2.0em"/>
                    Deep spiking neural networks are difficult to train. We instead convert deep neural networks from the analog domain to the spiking domain, with a small loss in accuracy.
                </div>
				</p>
           
	            <p style="height: 100px;">
                <a target="_blank" href="https://research.fb.com/publications/online-optical-marker-based-hand-tracking-with-deep-labels">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG18.jpg" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img></a>
				<div class="text">
                    Shangchen Han, Beibei Liu, Robert Wang, Yuting Ye, Chris D. Twigg, Kenrick Kin. 2018. <b>Online Optical Marker-based Hand Tracking with Deep Labels.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
                    <br/>[ 
                    <a target="_blank" href="https://research.fb.com/publications/online-optical-marker-based-hand-tracking-with-deep-labels">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SIG18_files/SIG18_Hands.pdf">Paper</a>&nbsp;
                    <a href="SIG18_files/SIG18_Hands.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/3197517.3201399?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="SIG18_files/SIG18_Hands.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="https://www.facebook.com/watch/?v=318250326599822">Video</a> &nbsp; &nbsp;
                    <a href="https://github.com/Beibei88/Mocap_SIG18_Data">Data</a>&nbsp;<img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We developed a deep-learning based marker labeling alogrithm for online capture of complex and dexterous hand motions, including hand-hand and hand-object interactions. 
                </div>
				</p>
                 
                <p style="height: 100px;">
                <a target="_blank" href="http://dartsim.github.io/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 78px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/dart_logo_big.jpg" style="position: relative; width: 160px; height: 70px; left: -182px; top: -15px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 78px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Jeongseok Lee, Michael X. Grey, Sehoon Ha, Tobias Kunz, Sumit Jain, Yuting Ye, Siddhartha S. Srinivasa, Mike Stilman, C. Karen Liu. 2018. <b>DART: Dynamic Animation and Robotics Toolkit.</b> <i>The Journal of Open Source Software</i>
                    <br/>[ 
                    <a target="_blank" href="http://dartsim.github.io/">Project</a>&nbsp;<img src="Research_files/GitHub.png" width="25" height="25" alt="Github" style="vertical-align:top"/> &nbsp; &nbsp;
                    <a href="http://joss.theoj.org/papers/10.21105/joss.00500">Paper</a> &nbsp; &nbsp;
                    <a href="Research_files/DART.bib">BibTeX</a>
                    ]<br style="line-height: 2.0em"/>
                </div>
				</p>
               
                <p style="height: 60px;">
                <a target="_blank" href="SCA15.html">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -50px; z-index: 1;"></img>
				<img src="Research_files/teaser_SCA15.jpg" style="position: relative; height: 120px; left: -182px; top: -60px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -50px;"></img></a>
				<div class="text">
                    Daniel Zimmermann, Stelian Coros, Yuting Ye, Bob Sumner, Markus Gross. 2015. <b>Hierarchical Planning and Control for Complex Motor Tasks.</b> <i>SCA '15 Proceedings of the ACM SIGGRAPH/Eurographics Symposuim on Computer Animation</i>
                    <br/>[ 
                    <a target="_blank" href="SCA15.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SCA15_files/SCA15_Control.pdf">Paper</a>&nbsp;
                    <a href="SCA15_files/SCA15_Control.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2786784.2786795?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="SCA15_files/SCA15.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="SCA15_files/SCA15_Control.mov">Video</a>
                    <a href="SCA15_files/SCA15_Control.mov"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="https://youtu.be/-gWH0o9UBbg"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We presented a planning and control framework that utilizes a hierarchy of dynamic models with increasing complexity and accuracy for complex tasks.  
                </div>
				</p>
                
                <p style="height: 100px;">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_texture.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
				<div class="text">
                    St&eacute;phane Grabli, Kevin Sprout, Yuting Ye. 2015. <b>Feature-Based Texture Stretch Compensation for 3D Meshes.</b> <i>ACM SIGGRAPH Talks</i>.
                    <br/>[ 
                    <a href="Research_files/SIG15_texture.pdf">Paper</a>&nbsp;
                    <a href="Research_files/SIG15_texture.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2775280.2792537?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="Research_files/SIG15_texture.bib">BibTeX</a>
                    ]<br style="line-height: 2.0em"/>
                    Stretching of rigid features on deforming skins usually betrays the synthetic nature of a CG creature. Our method mitigates such effects by re-parameterizing the texture coordinates to compensate for unwanted deformations on the 3D mesh. 
                </div>
                </p>
                
                <p style="height: 100px;">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 136px; width: 176px; left: -10px; top: -11px; z-index: 1;"></img>
				<img src="Research_files/teaser_transfer.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 136px; width: 176px; left: -354px; top: -11px;"></img>
				<div class="text">
                    Rachel Rose, Yuting Ye. 2015. <b>Multi-resolution Geometric Transfer for Jurassic World.</b> <i>ACM SIGGRAPH Talks</i>.
                    <br/>[ 
                    <a href="Research_files/SIG15_transfer.pdf">Paper</a>&nbsp;
                    <a href="Research_files/SIG15_transfer.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2775280.2792567?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="Research_files/SIG15_transfer.bib">BibTeX</a>
                    ]<br style="line-height: 2.0em"/>
                    We developed an easy-to-use workflow for artists to transfer geometric data between different mesh resolutions. Our tool enables efficient development of highly detailed creature assets such as the dinosaurs in "Jurassic World".
                </div>
                </p>

                <p style="height: 100px;">
                <a target="_blank" href="http://www.hao-li.com/Hao_Li/Hao_Li_-_publications_%5BRealtime_Facial_Animation_with_On-the-fly_Correctives%5D.html" style="color: #FFF">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG13.png" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
				<div class="text">
                    Hao Li, Jihun Yu, Yuting Ye, Chris Bregler. 2013. <b>Realtime Facial Animation with On-the-fly Correctives.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
                    <br/>[ 
                    <a target="_blank" href="http://www.hao-li.com/Hao_Li/Hao_Li_-_publications_%5BRealtime_Facial_Animation_with_On-the-fly_Correctives%5D.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SIG13_files/siggraph2013RTFAOC.pdf">Paper</a>&nbsp;
                    <a href="SIG13_files/siggraph2013RTFAOC.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2461912.2462019?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="Research_files/SIG13.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="http://www.hao-li.com/publications/movies/siggraph2013videoA.mov">Video</a>
                    <a href="http://www.hao-li.com/publications/movies/siggraph2013videoA.mov"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/X7y2RZdyZK0"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We developed a realtime facial tracking and retargeting system using an RGBD sensor. Our system produces accurate tracking results by continuously adapting to user specific expressions on-the-fly.
                </div>
				</p>
                
                <p style="height: 100px;">
                <a target="_blank" href="Research_files/SCA13_Contours.pdf" style="color: #FFF">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SCA13.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
				</a>
                <div class="text">
                    Kiran Bhat, Rony Goldenthal, Yuting Ye, Ronald Mallet, Michael Koperwas. 2013. <b>High Fidelity Facial Animation Capturing and Retargeting With Contours.</b> <i>SCA '13 Proceedings of the ACM SIGGRAPH/Eurographics Symposuim on Computer Animation</i>.
                    <br/>[ 
                    <a href="Research_files/SCA13_Contours.pdf">Paper</a>&nbsp;
                    <a href="Research_files/SCA13_Contours.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2485895.2485915?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" style="vertical-align:top"/></a> &nbsp;&nbsp; 
                    <a href="Research_files/SCA13.bib">BibTeX</a>  &nbsp;&nbsp;
                    <a href="https://www.dropbox.com/s/t5qmu19mdyuae00/SCA2013.mp4">Video</a>
                    <a href="https://www.dropbox.com/s/t5qmu19mdyuae00/SCA2013.mp4"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We developed a facial animation tracking system that utilizes eyelid and lip sillouettes for high fidelity results.
                </div>
                </p>

				<p style="height: 100px;">
                <a target="_blank" href="Research_files/2012_landing.pdf" style="color: #FFF">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIGA12.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
				<div class="text">
                    Sehoon Ha, Yuting Ye, C. Karen Liu. 2012. <b>Falling and Landing Motion Control for Character Animation.</b> <i>ACM Trans. Graph. (SIGGRAPH Asia)</i>
                    <br/>[ 
                    <a href="Research_files/2012_landing.pdf">Paper</a>&nbsp;
                    <a href="Research_files/2012_landing.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2366145.2366174?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="Research_files/SIGA12.bib">BibTeX</a>  &nbsp; &nbsp;
                    <a href="http://www.cc.gatech.edu/~sha9/projects/ha2012flm/2012_landing.mp4">Video</a>
                    <a href="http://www.cc.gatech.edu/~sha9/projects/ha2012flm/2012_landing.mp4"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/VN7Mb0AM6qo"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
				    We developed a general controller that allows a character to fall from a wide range of heights and initial velocities, continuously roll on the ground, and get back on feet.
                </div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="SIG12.html" style="color: #FFF">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG12.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
				<div class="text">
                    Yuting Ye, C. Karen Liu. 2012. <b>Synthesis of Detailed Hand Manipulations Using Contact Sampling.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
                    <br/>[ 
                    <a target="_blank" href="SIG12.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SIG12_files/SIG12ye_preprint.pdf">Paper</a>&nbsp;
                    <a href="SIG12_files/SIG12ye_preprint.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2185520.2185537?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp; 
                    <a href="SIG12_files/SIG12.bib">BibTeX</a>  &nbsp; &nbsp; 
                    <a href="https://www.dropbox.com/s/oi2akz3sye1h663/hand.avi">Video</a>
                    <a href="https://www.dropbox.com/s/oi2akz3sye1h663/hand.avi"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/x8c27XYTLTo"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
				    This work synthesizes detailed and physically plausible hand-object manipulations from captured motions of the full-body and objects. By sampling contact points between the hand and the object, we can efficiently discover complex finger gaits. 
                </div>
				</p>
            
                <p style="height: 100px;">
                <a target="_blank" href="SIG10.html" style="color: #FFF">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
                <img src="Research_files/teaser_SIG10.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
                <img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
                <div class="text">
                    Yuting Ye, C. Karen Liu. 2010. <b>Optimal Feedback Control for Character Animation Using an Abstract Model.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
                    <br/>[ 
                    <a target="_blank" href="SIG10.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SIG10_files/preprint.pdf">Paper</a>&nbsp;
                    <a href="SIG10_files/preprint.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/1833349.1778811?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp; 
                    <a href="SIG10_files/SIG10.bib">BibTeX</a>  &nbsp; &nbsp; 
                    <a href="SIG10_files/SIG10.divx">Video</a>
                    <a href="SIG10_files/SIG10.divx"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/3Lbmmoofu0E"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We presented an optimal feedback controller for a virtual character to follow a reference motion under physical perturbations and changes in the environment by replanning long-term goals and adjusting the motion timing on-the-fly.<br/>
                </div>
                </p>
            
                <p style="height: 100px;">
                <a target="_blank" href="EG10.html" style="color: #FFF">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
                <img src="Research_files/teaser_GP.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
                <img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
                <div class="text">
                    Yuting Ye, C. Karen Liu. 2010. <b>Synthesis of Responsive Motion Using a Dynamic Model.</b> <i>Computer Graphics Forum (Eurographics)</i>
                    <br/>[ 
                    <a target="_blank" href="EG10.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="EG10_files/preprint_EG10.pdf">Paper</a>&nbsp; 
                    <a href="EG10_files/preprint_EG10.pdf"><img src="Research_files/Preprint.png" width="25" height="23" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01625.x"><img src="Research_files/EG.jpg" width="25" height="20" alt="EG" style="vertical-align:middle"/></a> &nbsp; &nbsp;
                    <a href="EG10_files/EG10.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="EG10_files/demo_EG10.divx">Video</a>
                    <a href="EG10_files/demo_EG10.divx"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/0Q3WNbk0F9c"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We presented a nonlinear dimensionality reduction model for learning responsive behaviors from very few motion capture examples. Our model can synthesize physically plausible motions of a character responding to unexpected perturbations.<br/>
                </div>
                </p>

                <p style="height: 100px;">
                <a target="_blank" href="http://www.cc.gatech.edu/graphics/projects/Sumit/homepage/projects/phoward/index.html">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
                <img src="Research_files/teaser_TOG09.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
                <img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
                <div class="text">
                    Sumit Jain, Yuting Ye, C. Karen Liu. 2009. <b>Optimization-Based Interactive Motion Synthesis.</b> <i>ACM Trans. Graph. (TOG)</i>
                    <br/>[ 
                    <a target="_blank" href="http://www.cc.gatech.edu/graphics/projects/Sumit/homepage/projects/phoward/index.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="http://www.cc.gatech.edu/graphics/projects/Sumit/homepage/papers/tog09/jain_phoward_tog09.pdf">Paper</a>&nbsp;
                    <a href="http://www.cc.gatech.edu/graphics/projects/Sumit/homepage/papers/tog09/jain_phoward_tog09.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/1477926.1477936?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp; 
                    <a href="Research_files/TOG09.bib">BibTeX</a>  &nbsp; &nbsp;
                    <a href="https://www.dropbox.com/s/jmkez1tgk6agyut/SIG08_Final.avi">Video</a>
                    <a href="https://www.dropbox.com/s/jmkez1tgk6agyut/SIG08_Final.avi"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/S1ALeYCYDkc"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We presented a physics-based approach to synthesizing motions of a responsive virtual character in a dynamically varying environment. A constrained optimization problem that encodes high-level kinematic control strategies is solved at every time step.<br/> 
                </div>
                </p>

                <p style="height: 100px;">
                <a target="_blank" href="EigenPhysics.html">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
                <img src="Research_files/teaser_SIGA08.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
                <img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
                <div class="text">
                    Yuting Ye, C. Karen Liu. 2008. <b>Animating Responsive Characters with Dynamic Constraints in Near-Unactuated Coordinates.</b> <i>ACM Trans. Graph. (SIGGRAPH Asia)</i>
                    <br/>[ 
                    <a target="_blank" href="EigenPhysics.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="EigenPhysics_files/preprint.pdf">Paper</a>&nbsp;
                    <a href="EigenPhysics_files/preprint.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/1457515.1409065?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp; 
                    <a href="EigenPhysics_files/SIGA08.bib">BibTeX</a>  &nbsp; &nbsp; 
                    <a href="EigenPhysics_files/full.divx">Video</a>
                    <a href="EigenPhysics_files/full.divx"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/Ok6CDKQpQgE"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We developed a novel algorithm to animate physically responsive virtual characters by combining kinematic pose control with dynamic constraints in the joint actuation space.<br/>
                </div>
                </p>
                               
                <p style="height: 80px;">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 136px; width: 176px; left: -10px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG07.jpg" style="position: relative; height: 120px; left: -182px; top: -29px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 136px; width: 176px; left: -354px; top: -20px;"></img>
				<div class="text">
                    <br/>Sumit Jain, Yuting Ye, C. Karen Liu. 2007. <b>Optimization-based Interactive Motion Synthesis for Virtual Characters.</b> <i>ACM SIGGRAPH Sketches and Posters</i>. (Third place in Student Research Competition) 
                    <br/>[ 
                    <a href="Research_files/SIG07_Sketch.pdf">Sketch</a>&nbsp;
                    <a href="Research_files/SIG07_Sketch.pdf"><img src="Research_files/Preprint.png" width="25" height="25" style="vertical-align:top"/></a>  
                    <a href="https://dl.acm.org/doi/10.1145/1278780.1278828"><img src="Research_files/acm.png" width="25" height="25" style="vertical-align:top"/></a>  &nbsp; 
                    <a href="Research_files/SIG07_Poster.jpg">Poster</a>&nbsp;
                    <a href="Research_files/SIG07_Poster.jpg"><img src="Research_files/Preprint.png" width="25" height="25" style="vertical-align:top"/></a>  
                    <a href="https://dl.acm.org/doi/10.1145/1280720.1280777"><img src="Research_files/acm.png" width="25" height="25" alt="ACM DL" style="vertical-align:top"/></a>  &nbsp;
                    <a href="Research_files/SIG07.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="https://www.dropbox.com/s/j2snvwy15f453py/SIG07_Sketch.mp4">Video</a>
                    <a href="https://www.dropbox.com/s/j2snvwy15f453py/SIG07_Sketch.mp4"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]
                </div>
                </p>
            </div>

            <div class="item">
                <h1>Theses</h1>
            
                <p style="height: 120px;">
				<a href=ye_thesis.pdf>
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: 10px; z-index: 1;"></img>
				<img src="Research_files/gatech.jpg" style="position: relative; height: 120px; left: -182px; top: -0px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: 10px;"></img>
				</a>
                <div class="tighttext">
                    <br/>
                    <a href=ye_thesis.pdf><b>Simulation of characters with natural interactions</b></a>
                    <br/><i>Phd thesis</i>
                    <br/><i>Georgia Institute of Technology, 2012</i>
				</div>
                </p>
                
                <p style="height: 120px;">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 123px; width: 177px; left: -10px; top: 10px; z-index: 1;"></img>
				<img src="OtherProjects_files/capture.jpg" style="position: relative; width: 160px; left: -182px; top: -0px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 123px; width: 177px; left: -355px; top: 10px;"></img>
                <div class="tighttext">
                    <b>A momentum-based Bipedal Balance Controller</b>
                    <br/><i>Master's project</i>
                    <br/><i>University of Virginia, 2006</i>
                    <br/><a href="OtherProjects_files/A Momentum-based Bipedal Balance Controller.ppt">Presentation</a> (ppt, 1.09M)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="OtherProjects_files/video.zip">Video</a> (zip, 3.4M)
                </div>
				</p>

                <p style="height: 120px;">
				<img src="OtherProjects_files/pic1.jpg" style="position: relative; height: 138px; left: 15px; top: -0px; z-index: 1;"></img>
				<div class="tighttext">
                    <b>An Interactive 2D Vector Graphics Editing System</b>
				    <br/><i>Undergraduate thesis</i>
                    <br/><i>Peking University, 2004 </i>
				</div>
				</p>


            </div>
            
            <div class="item">
                <h1><a target="_blank" href="OtherProjects.html">Class projects</a></h1>
            </div>

        </div>
        
        <div class="footer">
        
            <span class="left">Copyright &copy; 2007 <a href="mailto:yutingye.public@gmail.com">Yuting Ye</a></span>
        
            <span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a href="http://arcsin.se/">Arcsin</a></span>

            <div class="clearer"><span></span></div>
        
        </div>

    </div>  

</div>

</body>

</html>
