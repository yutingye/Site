<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="content-type" content="text/html"/>
<meta name="description" content="Research work of Dr. Yuting Ye in computer graphics"/>
<meta name="keywords" content="Animation"/> 
<meta name="author" content="Yuting Ye"/> 
<link rel="stylesheet" type="text/css" href="default.css"/>
<title>Research</title>
</head>

<body>

<div class="main">

    <div class="container">

        <div class="gfx"><a><span></span></a></div>

        <div class="menu">
            <a href="index.html"><span>Home</span></a>
            <a href="Research.html"><span>Research</span></a>
            <a href="resume.html"><span>Resume</span></a>       
            <a href="yuting-cv.pdf"><span>CV (pdf)</span></a>
            <a id="last"><span> </span></a>
        </div>

        <div class="content">
            
            <div style="border: none; height: 0px; position: relative; top: -200px; left: 80px;">
                <div style="font: normal 20pt serif,'Times New Roman'; color: #777">Yuting Ye</div>
                <div style="font: normal 12pt serif,'Times New Roman'; color: #888; line-height: 1.5em">
                    Ph.D.<br/>
                    Researcher<br/>
                    yutingye.public AT gmail.com<br/>
                </div>
            </div>
            
            <!-- div class="item" -->
				<p>(* are advisor/co-advisors for internship projects)</p>
                <!-- p style="height: 120px;">
				<a target="_blank" href="">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 128px; width: 175px; left: -10px; top: 5px; z-index: 1;"/>
				<img src="Research_files/teaser_.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -180px; top: -5px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 128px; width: 175px; left: -350px; top: 5px;"/>
				</a>
				<div class="text">
					Authors. Year. <b>Title</b> <i>Venue</i>
					<br/>[
					<a target="_blank" href="">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="">Paper</a>&nbsp;&nbsp;&nbsp;
					<a href="">Code</a>
					]<br style="line-height: 2.0em"/>
					Brief description.
				</div>
				</p -->

				<div class="item">
				<h1>2024</h1>
                 
				<p style="height: 120px;">
				<a target="_blank" href="">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 178px; left: -10px; top: 5px; z-index: 1;"/>
				<img src="Research_files/teaser_SIG24_NMM.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -5px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 178px; left: -356px; top: 5px;"/>
				</a>
				<div class="text">
					Sebastian Starke, Paul Starke, Taku Komura, Yuting Ye. 2024. <b>Categorical Codebook Matching for Embodied Character Controllers</b>. <i>ACM Trans. Graph. (SIGGRAPH)</i>.
					<br/>[
					<a target="_blank" href="">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="">Paper</a>&nbsp;&nbsp;&nbsp;
					<a href="">Video</a>
					] (coming soon) <br style="line-height: 2.0em"/>
					We presented an end-to-end system that simultaneously learns a discrete latent codebook of motion and to sample from it based on input control. We showcase various applications in VR of controlling a full body character from sparse inputs.
				</div>
				</p>
               
				<p style="height: 100px;">
				<a target="_blank" href="">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 178px; left: -10px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_SIG24_WTD.png" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 178px; left: -356px; top: -10px;"/>
				</a>
				<div class="text">
					Peizhuo Li, Sebastian Starke*, Yuting Ye, Olga Sorkine-Hornung*. 2024. <b>WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds</b>. <i>SIGGRAPH Conference</i>.
					<br/>[
					<a target="_blank" href="">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="">Paper</a>&nbsp;&nbsp;&nbsp;
					<a href="">Video</a>
					] (coming soon) <br style="line-height: 2.0em"/>
					We achieved unsupervised semantic motion alignment between different skeleton morphologies via a shared and learned latent phase manifold. The key is to parameterize motion segments with a one-dimensional periodic phase and a discrete latent amplitude. 
				</div>
				</p>

				<p style="height: 100px;">
				<a target="_blank" href="https://arxiv.org/abs/2402.09211">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 178px; left: -10px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_EG24.png" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 178px; left: -356px; top: -10px;"/>
				</a>
				<div class="text">
					Dongseok Yang, Jiho Kang, Lingni Ma, Joseph Greer, Yuting Ye*, Sung-Hee Lee*. 2024. <b>DivaTrack: Diverse Motions and Bodies from Acceleration-enhanced Three-point Trackers</b>. <i>Eurographics</i> (<b>best paper honorable mention</b>).
					<br/>[
					<a target="_blank" href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.15057">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://arxiv.org/abs/2402.09211">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://diglib.eg.org/bitstreams/5b5c467f-0b1e-4f00-8e22-4b2fe9be6a56/download"><img src="Research_files/EG.jpg" width="25" height="25" alt="Eurographics" style="vertical-align:middle"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/NbmcB04hGiM?si=Y5azcJxsUADOJ9xX">Video
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>&nbsp;
					<a href="https://diglib.eg.org/bitstreams/51f2caea-a43d-481c-a0fe-54c756ed8ac5/download"><img src="Research_files/EG.jpg" width="25" height="25" alt="Eurographics" style="vertical-align:middle"/></a>
					]<br style="line-height: 2.0em"/>
					We incorporated IMUs in the 3-point body tracking problem, and highlighted better results in challenging scenarios of tracking diverse subjects and a wide range of actions.
				</div>
				</p>

				</div>
				</p>
                 
				<div class="item">
				<h1>2023</h1>
                 
				<p style="height: 120px;">
				<a target="_blank" href="https://stanford-tml.github.io/drop/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 136px; width: 176px; left: -9px; top: 5px; z-index: 1;"/>
				<img src="Research_files/teaser_Drop.png" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -181px; top: -4px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 136px; width: 176px; left: -353px; top: 5px;"/>
				</a>
				<div class="text">
					Yifeng Jiang, Jungdam Won, Yuting Ye*, C. Karen Liu*. 2023. <b>DROP: Dynamics Responses from Human Motion Prior and Projective Dynamics</b>. <i>SIGGRAPH Asia conference.</i>
					<br/>[
					<a target="_blank" href="https://stanford-tml.github.io/drop/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://arxiv.org/pdf/2309.13742.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3610548.3618175?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/tF5WW7qNMLI?si=Qu70PXDWMzDSex3c">Video
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We endowed generative motion models with dynamic responses to forces and collision by treating motion prior as one of the energy terms in a projective dynamics simulation.
				</div>
				</p>

				<p style="height: 100px;">
				<a target="_blank" href="https://dk-jang.github.io/MOCHA_SIGASIA2023/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 120px; width: 176px; left: -9px; top: -6px; z-index: 1;"/>
				<img src="Research_files/teaser_Mocha.png" alt="teaser image" style="position: relative; height: 105px; width: 160px; left: -182px; top: -14px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 120px; width: 176px; left: -353px; top: -6px;"/>
				</a>
				<div class="text">
					Deok-Kyeong Jang, Yuting Ye*, Jungdam Won, Sung-Hee Lee*. 2023. <b>MOCHA: Real-Time Motion Characterization via Context Matching</b> <i>SIGGRAPH Asia conference.</i>
					<br/>[
					<a target="_blank" href="https://dk-jang.github.io/MOCHA_SIGASIA2023/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://arxiv.org/pdf/2310.10079.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3610548.3618252?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/2nnEAwHMZNI?si=p0KjVrzdY9JnJbh1">Video
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We learned a style-agnostic context space with self-supervision for motion style transfer. In real time, we project the encoded input motion to the closest context latent, decode the context latent to a target style, then apply style transfer between input and target.
				</div>
				</p>
    				
				<p style="height: 100px;">
				<a target="_blank" href="https://rubbly.cn/publications/phaseMP/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 176px; left: -9px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_ICCV23.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 176px; left: -353px; top: -10px;"/>
				</a>
				<div class="text">
					Mingyi Shi, Sebastian Starke, Yuting Ye, Taku Komura*, Jungdam Won*. 2023. <b>PhaseMP: Robust 3D Pose Estimation via Phase-conditioned Human Motion Prior</b> <i>ICCV</i>
					<br/>[
					<a target="_blank" href="https://rubbly.cn/publications/phaseMP/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://rubbly.cn/publications/phaseMP/phaseMP_main.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://doi.ieeecomputersociety.org/10.1109/ICCV51070.2023.01353"><img src="Research_files/logo-ieee.png" width="32" height="22" alt="IEEE Explorer" style="vertical-align:middle"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/NKlHUY2Wn14?si=jcHZJosI7GgVYsEA">Video
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We presented a human motion prior model utilizing learned phase features. Progressing phases monotonically generates plausible poses under severe occlusion or noise.
				</div>
				</p>
    				
				<!-- p style="height: 100px;">
				<a target="_blank" href="https://dl.acm.org/toc/pacmcgit/2023/6/3">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 136px; width: 105px; left: 30px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_CGIT.png" alt="teaser image" style="position: relative; height: 120px; width: 95px; left: -74px; top: -19px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 136px; width: 105px; left: -178px; top: -10px;"/>
				</a>
				<div class="text">
					Huamin Wang, Yuting Ye, Victor Zordan. 2023. <b>Proceedings of the ACM on Computer Graphics and Interactive Techniques (PACMCGIT)</b>, <i>Vol. 6, issue 3.</i>
					<br/> [
					<a target="_blank" href="https://dl.acm.org/toc/pacmcgit/2023/6/3">Proceedings
					<img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a target="_blank" href="https://computeranimation.org/">SCA 2023</a>
					] <br style="line-height: 2.0em"/>
					All twenty-one technical papers from SCA 2023 are accepted to CGIT. They reflected state of the art in character animation and physics-based simulation, covering a wide range of topics such as agile and realistic characters and a full head of tightly coiled hair.
				</div>
				</p -->
        
				<p style="height: 100px;">
				<a target="_blank" href="https://www.cs.ubc.ca/~dreda/retargeting.html">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_SCA23.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"/>
				</a>
				<div class="text">
					Daniele Reda, Jungdam Won, Yuting Ye, Michiel van de Panne*, Alexander Winkler*. 2023. <b>Physics-based Motion Retargeting from Sparse Inputs.</b> <i>SCA</i>
					<br/>[
					<a target="_blank" href="https://www.cs.ubc.ca/~dreda/retargeting.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://arxiv.org/pdf/2307.01938.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3606928?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://www.youtube.com/watch?v=5D-DvX5scTk">Video
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We demonstrated a system for a user to puppeteer a bipedal ceature in VR without additional hardware accessories. We simiply replaced the human character with a creature in the QuestSim formula and applied only rudimentary kinematic retargeting.
				</div>
				</p>

				<p style="height: 100px;">
				<a target="_blank" href="https://research.facebook.com/publications/simulation_and_retargeting_of_complex_multi_character_interactions/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 137px; width: 180px; left: -10px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_SIG23_Retarget.jpg" alt="teaser image" style="position: relative; height: 120px; width: 163px; left: -185px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 137px; width: 180px; left: -361px; top: -10px;"/>
				</a>
				<div class="text">
					Yunbo Zhang, Deepak Gopinach, Yuting Ye, Jessica Hodgins, Greg Turk, Jungdam Won*. 2023. <b>Simulation and Retargeting of Complex Multi-Character Interactions.</b> <i>SIGGRAPH North America Conference</i>
					<br/>[
					<a target="_blank" href="https://research.facebook.com/publications/simulation_and_retargeting_of_complex_multi_character_interactions/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://research.facebook.com/file/119688477812149/Simulation_and_Retargeting_of_Complex_Multi_Character_Interactions.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3588432.3591491?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/iUsVCbI9OCI">Video
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We proposed an Interaction Graph representation that can not only simulate multi-character interactions, but also retarget to different proportions or even morphologies. 
				</div>
				</p>
                
				<p style="height: 100px;">
				<a target="_blank" href="https://sunny-codes.github.io/projects/questenvsim.html">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 187px; left: -16px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_SIG23_Env.jpg" alt="teaser image" style="position: relative; height: 120px; width: 167px; left: -196px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 187px; left: -378px; top: -10px;"/>
				</a>
				<div class="text">
					Sunmin Lee, Sebastian Starke, Yuting Ye, Jungdam Won*, Alexander W. Winkler*. 2023. <b>QuestEnvSim: Environment-aware Simulated Motion Tracking From Sparse Sensors.</b> <i>SIGGRAPH North America Conference</i>
					<br/>[
					<a target="_blank" href="https://sunny-codes.github.io/projects/questenvsim.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://arxiv.org/pdf/2306.05666.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/abs/10.1145/3588432.3591504?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://www.youtube.com/watch?v=HXkp3ILm5bY">Video
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We extended QuestSim to support environment interactions such as sitting on chairs and stepping over obstacles using an egocentric heighmap as an additional policy input.
				</div>
				</p>

                <p style="height: 100px;">
				<a target="_blank" href="https://research.facebook.com/publications/how-important-are-detailed-hand-motions-for-communication-for-a-virtual-character-through-the-lens-of-charades/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_TOG23.png" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"/>
				</a>
				<div class="text">
					Alex Adkins, Aline Normoyle, Lorraine Lin, Yun Sun, Yuting Ye, Max Di Luca, Sophie Jörg. 2023. <b>How important are Detailed Hand Motions for Communication for a Virtual Character Through the Lense of Charades?</b> <i>ACM Trans. Graph. (TOG)</i>
					<br/>[
					<a target="_blank" href="https://research.facebook.com/publications/how-important-are-detailed-hand-motions-for-communication-for-a-virtual-character-through-the-lens-of-charades/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://research.facebook.com/file/249413517518201/How-Important-are-Detailed-Hand-Motions-for-Communication-for-a-Virtual-Character-Through-the-Lens-of-Charades.pdf">Paper&nbsp;&nbsp;
						<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
						<a href="https://dl.acm.org/doi/pdf/10.1145/3578575?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We studied how artifacts in detailed hand motions affect the communication for a virtual character using the game of Charades, both in 2D videos and in VR.
				</div>
				</p>

                <p style="height: 100px;">
				<a target="_blank" href="https://alteredavatar.github.io/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 136px; width: 162px; left: -4px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_avatar.jpg" alt="teaser image" style="position: relative; height: 120px; width: 150px; left: -165px; top: -19px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 136px; width: 162px; left: -324px; top: -10px;"/>
				</a>
				<div class="text">
					Thu Nguyen-Phuoc, Gabriel Schwartz, Yuting Ye, Stephen Lombardi, Lei Xiao. 2023. <b>AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation.</b> <i>Arxiv</i>
					<br/>[
					<a target="_blank" href="https://alteredavatar.github.io/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://arxiv.org/pdf/2305.19245.pdf">Paper&nbsp;&nbsp;
						<img src="Research_files/arxiv.png" width="25" height="25" alt="Arxiv" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://alteredavatar.github.io/assets/videos/AA_Main_1080_lowRes.mp4">Video
					    <img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We devised a meta-learning algorithm to quickly adapt a dynamic 3D avatar (Instant Codec Avatar) to novel styles prompted by text, an example image, or both. Our reuslts preserve the subject identity across different expressions and views.
				</div>
				</p>

                <p style="height: 100px;">
				<a target="_blank" href="https://research.facebook.com/publications/learning-to-transfer-in-hand-manipulations-using-a-greedy-shape-curriculum/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 177px; left: -11px; top: -10px; z-index: 1;"/>
				<img src="Research_files/teaser_EG23.png" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -20px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 177px; left: -356px; top: -10px;"/>
				</a>
				<div class="text">
					Yunbo Zhang, Alex Clegg, Sehoon Ha, Greg Turk*, Yuting Ye*. 2023. <b>Learning to Transfer In-Hand Manipulations Using a Greedy Shape Curriculum.</b> <i>Eurographics</i>
					<br/>[
					<a target="_blank" href="https://research.facebook.com/publications/learning-to-transfer-in-hand-manipulations-using-a-greedy-shape-curriculum/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://research.facebook.com/file/3136290823184088/Eurographics_2023_Final.pdf">Paper&nbsp;&nbsp;
						<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
						<a href="https://diglib.eg.org/bitstream/handle/10.1111/cgf14741/v42i2pp025-036_cgf14741.pdf"><img src="Research_files/EG.jpg" width="25" height="20" alt="EG" style="vertical-align:middle"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://www.facebook.com/MetaResearch/videos/107501135652866/">Video
						<img src="Research_files/facebook.png" width="25" height="25" alt="Facebook Watch" style="vertical-align:middle"/></a>
						<a href="https://diglib.eg.org/bitstream/handle/10.1111/cgf14741/paper1047_mm.mp4?sequence=2&isAllowed=y"><img src="Research_files/EG.jpg" width="25" height="20" alt="EG" style="vertical-align:middle"/></a>
					]<br style="line-height: 2.0em"/>
					Our algorithm learns in-hand manipulation of novel objects from a motion capture demonstration. A novel "Greedy Shape Curriculum" is proposed to successfully transfer the input manipulation motion on a simple shape to various complex objects.  
				</div>
				</p>

                <p style="height: 100px;">
				<a target="_blank" href="https://openaccess.thecvf.com/content/WACV2023/html/Feng_Rethinking_the_Data_Annotation_Process_for_Multi-View_3D_Pose_Estimation_WACV_2023_paper.html">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 136px; width: 175px; left: -11px; top: -12px; z-index: 1;"/>
				<img src="Research_files/teaser_WACV23.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -21px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 136px; width: 175px; left: -354px; top: -12px;"/>
				</a>
				<div class="text">
					Qi Feng, Kun He*, He Wen, Cem Keskin, Yuting Ye. 2023. <b>Rethinking the Data Annotation Process for Multi-view 3D Pose Estimation with Active Learning and Self-Training.</b> <i>WACV</i>
					<br/>[
					<a target="_blank" href="https://openaccess.thecvf.com/content/WACV2023/html/Feng_Rethinking_the_Data_Annotation_Process_for_Multi-View_3D_Pose_Estimation_WACV_2023_paper.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://openaccess.thecvf.com/content/WACV2023/papers/Feng_Rethinking_the_Data_Annotation_Process_for_Multi-View_3D_Pose_Estimation_WACV_2023_paper.pdf">Paper&nbsp;&nbsp;
						<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
						<a href="https://doi.ieeecomputersociety.org/10.1109/WACV56688.2023.00565"><img src="Research_files/logo-ieee.png" width="32" height="22" alt="IEEE Explorer" style="vertical-align:middle"/></a>&nbsp;&nbsp;&nbsp;
					<a href="https://github.com/facebookresearch/multi_view_active_learning">Code
						<img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We devised an active learning strategy to reduce manual annotation effort for 3D pose estimation problems using multi-view consistency and self-training with pseudo labels.
				</div>
				</p>
				</div>

				<div class="item">
				<h1>2022</h1>

                <p style="height: 120px;">
				<a target="_blank" href="https://research.facebook.com/publications/questsim-human-motion-tracking-from-sparse-sensors-with-simulated-avatars/">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 178px; left: -10px; top:5px; z-index: 1;"/>
				<img src="Research_files/teaser_SIGA22_QS.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -5px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 178px; left: -356px; top: 5px;"/>
				</a>
				<div class="text">
					Alexander W. Winkler, Jungdam Won, Yuting Ye. 2022. <b>QuestSim: Human Motion Tracking From Sparse Sensors With Simulated Avatars.</b> <i>SIGGRAPH Asia Conference</i>
					<br/>[
					<a target="_blank" href="https://research.facebook.com/publications/questsim-human-motion-tracking-from-sparse-sensors-with-simulated-avatars/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://research.facebook.com/file/500550158623023/QuestSim--Human-Motion-Tracking-from-Sparse-Sensors-with-Simulated-Avatars.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/abs/10.1145/3550469.3555411?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/CkTHsz6Ldas">Video 
					<img src="Research_files/youtube.png" width="25" height="25" alt="Youtube video" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We apply deep reinforcement learning to control a simulated full body avatar in real time. The avatar is driven only by input signals from a VR headset and two controllers, and it is able to reliably follow the user's motion in a variety of activities.
				</div>
				</p>

                <p style="height: 100px;">
				<a target="_blank" href="https://tml.stanford.edu/publications/2022/transformer-inertial-poser-real-time-human-motion-reconstruction-sparse-imus">
				<img src="Research_files/imageEffectsAbove.png" alt="" style="position: relative; height: 138px; width: 178px; left: -11px; top: -15px; z-index: 1;"/>
				<img src="Research_files/teaser_SIGA22_TIP.jpg" alt="teaser image" style="position: relative; height: 120px; width: 160px; left: -183px; top: -25px; z-index: 1;"/>
				<img src="Research_files/imageEffectsBelow.png" alt="" style="position: relative; height: 138px; width: 178px; left: -357px; top: -15px;"/>
				</a>
				<div class="text">
					Yifeng Jiang, Yuting Ye*, Deepak Gopinath, Jungdam Won, Alexander W. Winkler, C. Karen Liu*. 2022. <b>Transformer Inertial Poser: Real-time Human Motion Reconstruction From Sparse IMUs With Simultaneous Terrain Generation.</b> <i>SIGGRAPH Asia Conference</i>
					<br/>[
					<a target="_blank" href="https://tml.stanford.edu/publications/2022/transformer-inertial-poser-real-time-human-motion-reconstruction-sparse-imus">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://arxiv.org/pdf/2203.15720.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3550469.3555428?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/rXb6SaXsnc0">Video 
                    <img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://github.com/jyf588/transformer-inertial-poser">Code 
					<img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a>
					]<br style="line-height: 2.0em"/>
					We reconstruct both the human motion and the environment from 6 body-worn IMUs.
				</div>
				</p>
				</div>

				<div class="item">
				<h1>2021</h1>

				<p style="height: 120px;">
				<a target="_blank" href="https://research.fb.com/publications/evaluating-grasping-visualizations-and-control-modes-in-a-vr-game/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 178px; left: -10px; top: 5px; z-index: 1;"></img>
				<img src="Research_files/teaser_TAP21.jpg" style="position: relative; height: 120px; width: 160px; left: -183px; top: -5px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 178px; left: -356px; top: 5px;"></img></a>
				<div class="text">
					 Alex Adkins, Lorraine Lin, Aline Normoyle, Ryan Canales, Yuting Ye, Sophie Jörg. 2021. <b>Evaluating Grasping Visualizations and Control Modes in a VR Game.</b> <i>ACM Transactions on Applied Perception (TAP) </i>
					<br/>[
					<a target="_blank" href="https://research.fb.com/publications/evaluating-grasping-visualizations-and-control-modes-in-a-vr-game/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://research.facebook.com/file/933367327282604/Evaluating-Grasping-Visualizations-and-Control-Modes-in-a-VR-Game.pdf">Paper&nbsp;&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/>
					<a href="https://dl.acm.org/doi/10.1145/3486582?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;
					<a href="https://www.facebook.com/watch/?v=220461743440901">Video
						<img src="Research_files/facebook.png" width="25" height="25" alt="Facebook Watch" style="vertical-align:middle"/></a>
					]<br style="line-height: 2.0em"/>
					We built a VR Escape Room game to study virtual hand-object manipulation. Immersive virtual environment is an important factor in VR perception studies.
				</div>
				</p>

				<p style="height: 100px;">
				<a target="_blank" href="https://github.com/cghezhang/ManipNet">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 178px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG21.jpg" style="position: relative; height: 120px; width: 160px; left: -183px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 178px; left: -356px; top: -10px;"></img></a>
				<div class="text">
					He Zhang, Yuting Ye*, Takaaki Shiratori, Taku Komura*. 2021. <b>ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
					<br/>[
					<a target="_blank" href="https://github.com/cghezhang/ManipNet">Project with Code
					<img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="http://www.ipab.inf.ed.ac.uk/cgvu/zhang2021.pdf">Paper&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3450626.3459830?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://youtu.be/xvAq8ax5cuE">Video
					<img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>&nbsp;
					]<br style="line-height: 2.0em"/>
					We synthesize detailed finger motions for in-hand object manipulation from trajctories of the wrists and the objects, by representing their geometric relationship.
				</div>
				</p>

				<p style="height: 100px;">
				<a target="_blank" href="https://arxiv.org/abs/2109.15299">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 137px; width: 176px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_diform.jpg" style="position: relative; height: 122px; width: 160px; left: -182px; top: -19px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 137px; width: 176px; left: -354px; top: -10px;"></img></a>
				<div class="text">
					Binbin Xu, Lingni Ma*, Yuting Ye, Tanner Schmidt, Christopher D. Twigg, Steven Lovegrove. 2021. <b>Identity-Disentangled Neural Deformation Model for Dynamic Meshes.</b> <i>Arxiv: 2109.15299</i>
					<br/>[
					<a href="https://arxiv.org/pdf/2109.15299.pdf">Paper&nbsp;
					<img src="Research_files/arxiv.png" width="20" height="20" style="vertical-align:top"/></a>&nbsp;
					]<br style="line-height: 2.0em"/>
					We learn a Deep-SDF like neural deformation model with disentangled identity and shape latent space from unregistered and partial 4D scans for pose and shape estimation.
				</div>
				</p>
				</div>
	
				<div class="item">
				<h1>2020</h1>
			
				<p style="height: 120px;">
                <a target="_blank" href="https://zhouyisjtu.github.io/project_vcmeshcnn/vcmeshcnn.html">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 128px; width: 175px; left: -10px; top: 7px; z-index: 1;"></img>
				<img src="Research_files/highres_body.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: 2px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 128px; width: 175px; left: -353px; top: 7px;"></img></a>
				<div class="text">
                    Yi Zhou, Chenglei Wu*, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li*, Yaser Sheikh. 2020. <b>Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying Kernels.</b> <i>NeurIPS</i>
                    <br/>[ 
                    <a target="_blank" href="https://zhouyisjtu.github.io/project_vcmeshcnn/vcmeshcnn.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://arxiv.org/pdf/2006.04325.pdf">Paper</a> &nbsp; &nbsp;
                    <a href="https://github.com/facebookresearch/VCMeshConv">Code</a>
                    <a href="https://github.com/facebookresearch/VCMeshConv"><img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a> 
                    ]<br style="line-height: 2.0em"/>
					Mesh convolution is challenging due to irregular local connectivities. We proposed a novel convolution operator with globally shared weights and varying local coefficients.
				</div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="https://dl.acm.org/doi/10.1145/3415263.3419155">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/HandsCourse.jpg" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img></a>
				<div class="text">
                    Sophie Jörg, Yuting Ye, Michael Neff, Franziska Mueller, Victor Zordan. 2020. <b>Virtual hands in VR: motion capture, synthesis, and perception.</b> <i>SIGGRAPH and SIGGRAPH Asia Courses</i>.&nbsp; &nbsp; [ 
					<a target="_blank" href="https://www.dropbox.com/sh/3rnptvg27aifmda/AADD-7gIj6kri4ZjOunav3mqa">Course Notes</a> &nbsp; &nbsp;
					<a target="_blank" href="https://dl.acm.org/doi/10.1145/3415263.3419155?cid=81335499985">Presentation
					<img src="Research_files/dlauthorizer.jpg" width="20" height="20" alt="ACM Digital Library" style="vertical-align:top"/></a> ]<br/>
					In this course, we cover state-of-the-art methods of hand representation and animation in virtual reality. Topics include finger motion capture hardware and algorithms, hand animation with or without physics, and hand perceptions in VR.
				</div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="https://research.fb.com/publications/megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG20.jpg" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img></a>
				<div class="text">
                    Shangchen Han, Beibei Liu, Randi Cabezas, Christopher D Twigg, Peizhao Zhang, Jeff Petkau, Tsz-Ho Yu, Chun-Jung Tai, Muzaffer Akbay, Zheng Wang, Asaf Nitzan, Gang Dong, Yuting Ye, Lingling Tao, Chengde Wan, Robert Wang. 2020. <b>MEgATrack: monochrome egocentric articulated hand-tracking for virtual reality.</b> <i>SIGGRAPH</i>
                    <br/>[ 
                    <a target="_blank" href="https://research.fb.com/publications/megatrack-monochrome-egocentric-articulated-hand-tracking-for-virtual-reality/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://research.facebook.com/file/977630383019036/MEgATrack-Monochrome-Egocentric-Articulated-Hand-Tracking-for-Virtual-Reality.pdf">Paper&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3386569.3392452?cid=81335499985">
					<img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a> &nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.facebook.com/watch/?v=227793185857989">Video
						<img src="Research_files/facebook.png" width="25" height="25" alt="Facebook Watch" style="vertical-align:middle"/></a>
                    ]<br style="line-height: 2.0em"/>
					Hand tracking from monochrome cameras on the standalone Quest VR headsets.
				</div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="https://www.bmvc2020-conference.com/conference/papers/paper_0481.html">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 130px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/Global.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -15px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 130px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Julian Habekost, Takaaki Shiratori*, Yuting Ye and Taku Komura*. 2020. <b>Learning 3D Global Human Motion Estimation from Unpaired, Disjoint Datasets.</b> <i>BMVC</i>
                    <br/>[ 
                    <a target="_blank" href="https://www.bmvc2020-conference.com/conference/papers/paper_0481.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.bmvc2020-conference.com/assets/papers/0481.pdf">Paper</a> &nbsp; &nbsp;
                    <a href="https://youtu.be/ZuE78scOVOQ">Video</a>
                    <a href="https://youtu.be/ZuE78scOVOQ"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a> 
                    ]<br style="line-height: 2.0em"/>
                	Natural human motions are constrained to maintain contacts and body size over time. These are useful information to estimate global motions from monocular videos. We show how to learn such prior from a generic motion database independent of the videos.  
				</div>
				</p>
				</div>
			
				<div class="item">
				<h1>2019</h1>
				
				<p style="height: 120px;">
                <a target="_blank" href="https://arxiv.org/abs/1904.07528">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 135px; width: 175px; left: -10px; top: 5px; z-index: 1;"></img>
				<img src="Research_files/Disentangle.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -3px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 135px; width: 175px; left: -353px; top: 5px;"></img></a>
				<div class="text">
                    Yikang Li, Chris Twigg*, Yuting Ye, Lingling Tao, Xiaogang Wang. 2019. <b>Disentangling Pose from Appearance in Monochrome Hand Images.</b> <i>ICCV <a target="_blank" href="https://sites.google.com/view/hands2019/home">Hands</a> Workshop</i>
                    <br/>[ 
                    <a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/HANDS/LI_Disentangling_Pose_from_Appearance_in_Monochrome_Hand_Images_ICCVW_2019_paper.pdf">Paper&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://doi.ieeecomputersociety.org/10.1109/ICCVW.2019.00346"><img src="Research_files/logo-ieee.png" width="32" height="22" alt="IEEE Explorer" style="vertical-align:middle"/></a>
                    ]<br style="line-height: 2.0em"/>
                	We explored self-supervised learning to disentangle the hand pose from its appearance in monochrome images, and showed improved pose estimation robustness.
				</div>
				</p>
            
                <p style="height: 100px;">
                <a target="_blank" href="https://rcanale827d.myportfolio.com/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 130px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/Grasp.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -18px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 130px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Ryan Canales, Aline Normoyle, Yu Sun, Yuting Ye, Massimiliano Di Luca, Sophie Jörg. 2019. <b>Virtual Grasping Feedback and Virtual Hand Ownership.</b> <i>ACM SAP</i>
                    <br/>[ 
                    <a target="_blank" href="https://rcanale827d.myportfolio.com/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://docs.wixstatic.com/ugd/fd7532_889bdfdc831b4ca7b98ee4878adb3afd.pdf">Paper&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://dl.acm.org/doi/10.1145/3343036.3343132?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM Digital Library" style="vertical-align:top"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://youtu.be/hI8joIgPwUM">Video</a>
                    <a href="https://youtu.be/hI8joIgPwUM"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a> 
                    ]<br style="line-height: 2.0em"/>
                	We experimented with different visual styles for grasping virtual objects in VR. We found that showing the real hand poses with object penetration helps grasping efficiency, but users prefer physically consistent penetration-free poses.
				</div>
				</p>

                <p style="height: 100px;">
                <a target="_blank" href="https://www.lorrainelin.com/vrpuzzlegame">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/toyblocks_burger.png" style="position: relative; height: 120px; width: 160px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img></a>
				<div class="text">
                    Lorraine Lin, Aline Normoyle, Alexandra Adkins, Yun Sun, Andrew Robb, Yuting Ye, Max Di Luca, Sophie Jörg. 2019. <b>The Effect of Hand Size and Interaction Modality on the Virtual Hand Illusion.</b> <i>IEEE VR</i>
                    <br/>[ 
                    <a target="_blank" href="https://www.lorrainelin.com/vrpuzzlegame">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.lorrainelin.com/s/2019_Lin_et_al_Hand_Size_and_Interaction_Modality.pdf">Paper&nbsp;
					<img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a>
					<a href="https://ieeexplore.ieee.org/document/8797787"><img src="Research_files/logo-ieee.png" width="32" height="22" alt="IEEE Explorer" style="vertical-align:middle"/></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://youtu.be/soPGP7AiC9Q">Video</a>
                    <a href="https://youtu.be/soPGP7AiC9Q"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a> 
                    ]<br style="line-height: 2.0em"/>
                    Our VR experiment found that a matching hand size with dextrous finger tracking are more preferrable for most users, but other conditions still provide a fun experience.
                </div>
				</p>
    
                <p style="height: 100px;">
                <a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6416793/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 130px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/SNN.jpg" style="position: relative; height: 120px; width: 160px; left: -182px; top: -17px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 130px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Abhronil Sengupta, Yuting Ye*, Robert Wang, Chiao Liu, Kaushik Roy. 2019. <b>Going Deeper in Spiking Neural Networks: VGG and Residual Architectures.</b> <i>Frontiers in Neuroscience</i>
                    <br/>[ 
                    <a target="_blank" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6416793/">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6416793/pdf/fnins-13-00095.pdf">Paper</a>
                    ]<br style="line-height: 2.0em"/>
                    Deep spiking neural networks are difficult to train. We instead convert deep neural networks from the analog domain to the spiking domain, with a small loss in accuracy.
                </div>
				</p>
          		</div>

		  		<div class="item">
				<h1>2018</h1>

	            <p style="height: 120px;">
                <a target="_blank" href="https://research.fb.com/publications/online-optical-marker-based-hand-tracking-with-deep-labels">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: 5px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG18.jpg" style="position: relative; height: 120px; width: 160px; left: -182px; top: -5px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: 5px;"></img></a>
				<div class="text">
                    Shangchen Han, Beibei Liu, Robert Wang, Yuting Ye, Chris D. Twigg, Kenrick Kin. 2018. <b>Online Optical Marker-based Hand Tracking with Deep Labels.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
                    <br/>[ 
                    <a target="_blank" href="https://research.fb.com/publications/online-optical-marker-based-hand-tracking-with-deep-labels">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SIG18_files/SIG18_Hands.pdf">Paper</a>&nbsp;
                    <a href="SIG18_files/SIG18_Hands.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/3197517.3201399?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="SIG18_files/SIG18_Hands.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="https://www.facebook.com/watch/?v=318250326599822">Video
						<img src="Research_files/facebook.png" width="25" height="25" alt="Facebook Watch" style="vertical-align:middle"/></a>&nbsp;&nbsp;
                    <a href="https://github.com/Beibei88/Mocap_SIG18_Data">Data</a>&nbsp;<img src="Research_files/GitHub.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We developed a deep-learning based marker labeling alogrithm for online capture of complex and dexterous hand motions, including hand-hand and hand-object interactions. 
                </div>
				</p>
                 
                <p style="height: 100px;">
                <a target="_blank" href="http://dartsim.github.io/">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 78px; width: 175px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/dart_logo_big.jpg" style="position: relative; width: 160px; height: 70px; left: -182px; top: -15px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 78px; width: 175px; left: -353px; top: -10px;"></img></a>
				<div class="text">
                    Jeongseok Lee, Michael X. Grey, Sehoon Ha, Tobias Kunz, Sumit Jain, Yuting Ye, Siddhartha S. Srinivasa, Mike Stilman, C. Karen Liu. 2018. <b>DART: Dynamic Animation and Robotics Toolkit.</b> <i>The Journal of Open Source Software</i>
                    <br/>[ 
                    <a target="_blank" href="http://dartsim.github.io/">Project</a>&nbsp;<img src="Research_files/GitHub.png" width="25" height="25" alt="Github" style="vertical-align:top"/> &nbsp; &nbsp;
                    <a href="http://joss.theoj.org/papers/10.21105/joss.00500">Paper</a> &nbsp; &nbsp;
                    <a href="Research_files/DART.bib">BibTeX</a>
                    ]<br style="line-height: 2.0em"/>
                </div>
				</p>
            	</div>

				<div class="item">
				<h1>2015</h1>

                <p style="height: 120px;">
                <a target="_blank" href="SCA15.html" style="color: #FFF">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: 0px; z-index: 1;"></img>
				<img src="Research_files/teaser_SCA15.jpg" style="position: relative; height: 120px; left: -182px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: 0px;"></img></a>
				<div class="text">
                    Daniel Zimmermann, Stelian Coros, Yuting Ye, Bob Sumner, Markus Gross. 2015. <b>Hierarchical Planning and Control for Complex Motor Tasks.</b> <i>SCA '15 Proceedings of the ACM SIGGRAPH/Eurographics Symposuim on Computer Animation</i>
                    <br/>[ 
                    <a target="_blank" href="SCA15.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SCA15_files/SCA15_Control.pdf">Paper</a>&nbsp;
                    <a href="SCA15_files/SCA15_Control.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2786784.2786795?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="SCA15_files/SCA15.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="SCA15_files/SCA15_Control.mov">Video</a>
                    <a href="SCA15_files/SCA15_Control.mov"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="https://youtu.be/-gWH0o9UBbg"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We presented a planning and control framework that utilizes a hierarchy of dynamic models with increasing complexity and accuracy for complex tasks.  
                </div>
				</p>
                
                <p style="height: 100px;">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_texture.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
				<div class="text">
                    St&eacute;phane Grabli, Kevin Sprout, Yuting Ye. 2015. <b>Feature-Based Texture Stretch Compensation for 3D Meshes.</b> <i>ACM SIGGRAPH Talks</i>.
                    <br/>[ 
                    <a href="Research_files/SIG15_texture.pdf">Paper</a>&nbsp;
                    <a href="Research_files/SIG15_texture.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2775280.2792537?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="Research_files/SIG15_texture.bib">BibTeX</a>
                    ]<br style="line-height: 2.0em"/>
                    Stretching of rigid features on deforming skins usually betrays the synthetic nature of a CG creature. Our method mitigates such effects by re-parameterizing the texture coordinates to compensate for unwanted deformations on the 3D mesh. 
                </div>
                </p>
                
                <p style="height: 100px;">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 136px; width: 176px; left: -10px; top: -11px; z-index: 1;"></img>
				<img src="Research_files/teaser_transfer.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 136px; width: 176px; left: -354px; top: -11px;"></img>
				<div class="text">
                    Rachel Rose, Yuting Ye. 2015. <b>Multi-resolution Geometric Transfer for Jurassic World.</b> <i>ACM SIGGRAPH Talks</i>.
                    <br/>[ 
                    <a href="Research_files/SIG15_transfer.pdf">Paper</a>&nbsp;
                    <a href="Research_files/SIG15_transfer.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2775280.2792567?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="Research_files/SIG15_transfer.bib">BibTeX</a>
                    ]<br style="line-height: 2.0em"/>
                    We developed an easy-to-use workflow for artists to transfer geometric data between different mesh resolutions. Our tool enables efficient development of highly detailed creature assets such as the dinosaurs in "Jurassic World".
                </div>
                </p>
				</div>

				<div class="item">
				<h1>2013</h1>

                <p style="height: 120px;">
                <a target="_blank" href="http://www.hao-li.com/Hao_Li/Hao_Li_-_publications_%5BRealtime_Facial_Animation_with_On-the-fly_Correctives%5D.html" style="color: #FFF">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: 5px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG13.png" style="position: relative; height: 120px; left: -182px; top: -5px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: 5px;"></img>
                </a>
				<div class="text">
                    Hao Li, Jihun Yu, Yuting Ye, Chris Bregler. 2013. <b>Realtime Facial Animation with On-the-fly Correctives.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
                    <br/>[ 
                    <a target="_blank" href="http://www.hao-li.com/Hao_Li/Hao_Li_-_publications_%5BRealtime_Facial_Animation_with_On-the-fly_Correctives%5D.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SIG13_files/siggraph2013RTFAOC.pdf">Paper</a>&nbsp;
                    <a href="SIG13_files/siggraph2013RTFAOC.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2461912.2462019?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="Research_files/SIG13.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="http://www.hao-li.com/publications/movies/siggraph2013videoA.mov">Video</a>
                    <a href="http://www.hao-li.com/publications/movies/siggraph2013videoA.mov"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/X7y2RZdyZK0"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We developed a realtime facial tracking and retargeting system using an RGBD sensor. Our system produces accurate tracking results by continuously adapting to user specific expressions on-the-fly.
                </div>
				</p>
                
                <p style="height: 100px;">
                <a target="_blank" href="Research_files/SCA13_Contours.pdf" style="color: #FFF">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SCA13.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
				</a>
                <div class="text">
                    Kiran Bhat, Rony Goldenthal, Yuting Ye, Ronald Mallet, Michael Koperwas. 2013. <b>High Fidelity Facial Animation Capturing and Retargeting With Contours.</b> <i>SCA '13 Proceedings of the ACM SIGGRAPH/Eurographics Symposuim on Computer Animation</i>.
                    <br/>[ 
                    <a href="Research_files/SCA13_Contours.pdf">Paper</a>&nbsp;
                    <a href="Research_files/SCA13_Contours.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2485895.2485915?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" style="vertical-align:top"/></a> &nbsp;&nbsp; 
                    <a href="Research_files/SCA13.bib">BibTeX</a>  &nbsp;&nbsp;
                    <a href="https://www.dropbox.com/s/t5qmu19mdyuae00/SCA2013.mp4">Video</a>
                    <a href="https://www.dropbox.com/s/t5qmu19mdyuae00/SCA2013.mp4"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We developed a facial animation tracking system that utilizes eyelid and lip sillouettes for high fidelity results.
                </div>
                </p>
				</div>

				<div class="item">
				<h1>2012 and before</h1>

				<p style="height: 120px;">
                <a target="_blank" href="Research_files/2012_landing.pdf" style="color: #FFF">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: 5px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIGA12.jpg" style="position: relative; height: 120px; left: -182px; top: -5px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: 5px;"></img>
                </a>
				<div class="text">
                    Sehoon Ha, Yuting Ye, C. Karen Liu. 2012. <b>Falling and Landing Motion Control for Character Animation.</b> <i>ACM Trans. Graph. (SIGGRAPH Asia)</i>
                    <br/>[ 
                    <a href="Research_files/2012_landing.pdf">Paper</a>&nbsp;
                    <a href="Research_files/2012_landing.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2366145.2366174?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp;
                    <a href="Research_files/SIGA12.bib">BibTeX</a>  &nbsp; &nbsp;
                    <a href="http://www.cc.gatech.edu/~sha9/projects/ha2012flm/2012_landing.mp4">Video</a>
                    <a href="http://www.cc.gatech.edu/~sha9/projects/ha2012flm/2012_landing.mp4"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/VN7Mb0AM6qo"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
				    We developed a general controller that allows a character to fall from a wide range of heights and initial velocities, continuously roll on the ground, and get back on feet.
                </div>
				</p>

				<p style="height: 100px;">
                <a target="_blank" href="SIG12.html" style="color: #FFF">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG12.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
				<div class="text">
                    Yuting Ye, C. Karen Liu. 2012. <b>Synthesis of Detailed Hand Manipulations Using Contact Sampling.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
                    <br/>[ 
                    <a target="_blank" href="SIG12.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SIG12_files/SIG12ye_preprint.pdf">Paper</a>&nbsp;
                    <a href="SIG12_files/SIG12ye_preprint.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/2185520.2185537?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp; 
                    <a href="SIG12_files/SIG12.bib">BibTeX</a>  &nbsp; &nbsp; 
                    <a href="https://www.dropbox.com/s/oi2akz3sye1h663/hand.avi">Video</a>
                    <a href="https://www.dropbox.com/s/oi2akz3sye1h663/hand.avi"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/x8c27XYTLTo"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
				    This work synthesizes detailed and physically plausible hand-object manipulations from captured motions of the full-body and objects. By sampling contact points between the hand and the object, we can efficiently discover complex finger gaits. 
                </div>
				</p>
            
                <p style="height: 100px;">
                <a target="_blank" href="SIG10.html" style="color: #FFF">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
                <img src="Research_files/teaser_SIG10.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
                <img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
                <div class="text">
                    Yuting Ye, C. Karen Liu. 2010. <b>Optimal Feedback Control for Character Animation Using an Abstract Model.</b> <i>ACM Trans. Graph. (SIGGRAPH)</i>
                    <br/>[ 
                    <a target="_blank" href="SIG10.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="SIG10_files/preprint.pdf">Paper</a>&nbsp;
                    <a href="SIG10_files/preprint.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/1833349.1778811?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp; 
                    <a href="SIG10_files/SIG10.bib">BibTeX</a>  &nbsp; &nbsp; 
                    <a href="SIG10_files/SIG10.divx">Video</a>
                    <a href="SIG10_files/SIG10.divx"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/3Lbmmoofu0E"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We presented an optimal feedback controller for a virtual character to follow a reference motion under physical perturbations and changes in the environment by replanning long-term goals and adjusting the motion timing on-the-fly.<br/>
                </div>
                </p>
            
                <p style="height: 100px;">
                <a target="_blank" href="EG10.html" style="color: #FFF">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
                <img src="Research_files/teaser_GP.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
                <img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
                <div class="text">
                    Yuting Ye, C. Karen Liu. 2010. <b>Synthesis of Responsive Motion Using a Dynamic Model.</b> <i>Computer Graphics Forum (Eurographics)</i>
                    <br/>[ 
                    <a target="_blank" href="EG10.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="EG10_files/preprint_EG10.pdf">Paper</a>&nbsp; 
                    <a href="EG10_files/preprint_EG10.pdf"><img src="Research_files/Preprint.png" width="25" height="23" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2009.01625.x"><img src="Research_files/EG.jpg" width="25" height="20" alt="EG" style="vertical-align:middle"/></a> &nbsp; &nbsp;
                    <a href="EG10_files/EG10.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="EG10_files/demo_EG10.divx">Video</a>
                    <a href="EG10_files/demo_EG10.divx"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/0Q3WNbk0F9c"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We presented a nonlinear dimensionality reduction model for learning responsive behaviors from very few motion capture examples. Our model can synthesize physically plausible motions of a character responding to unexpected perturbations.<br/>
                </div>
                </p>

                <p style="height: 100px;">
                <a target="_blank" href="http://www.cc.gatech.edu/graphics/projects/Sumit/homepage/projects/phoward/index.html">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
                <img src="Research_files/teaser_TOG09.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
                <img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
                <div class="text">
                    Sumit Jain, Yuting Ye, C. Karen Liu. 2009. <b>Optimization-Based Interactive Motion Synthesis.</b> <i>ACM Trans. Graph. (TOG)</i>
                    <br/>[ 
                    <a target="_blank" href="http://www.cc.gatech.edu/graphics/projects/Sumit/homepage/projects/phoward/index.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="http://www.cc.gatech.edu/graphics/projects/Sumit/homepage/papers/tog09/jain_phoward_tog09.pdf">Paper</a>&nbsp;
                    <a href="http://www.cc.gatech.edu/graphics/projects/Sumit/homepage/papers/tog09/jain_phoward_tog09.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/1477926.1477936?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp; 
                    <a href="Research_files/TOG09.bib">BibTeX</a>  &nbsp; &nbsp;
                    <a href="https://www.dropbox.com/s/jmkez1tgk6agyut/SIG08_Final.avi">Video</a>
                    <a href="https://www.dropbox.com/s/jmkez1tgk6agyut/SIG08_Final.avi"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/S1ALeYCYDkc"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We presented a physics-based approach to synthesizing motions of a responsive virtual character in a dynamically varying environment. A constrained optimization problem that encodes high-level kinematic control strategies is solved at every time step.<br/> 
                </div>
                </p>

                <p style="height: 100px;">
                <a target="_blank" href="EigenPhysics.html">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: -10px; z-index: 1;"></img>
                <img src="Research_files/teaser_SIGA08.jpg" style="position: relative; height: 120px; left: -182px; top: -20px; z-index: 1;"></img>
                <img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: -10px;"></img>
                </a>
                <div class="text">
                    Yuting Ye, C. Karen Liu. 2008. <b>Animating Responsive Characters with Dynamic Constraints in Near-Unactuated Coordinates.</b> <i>ACM Trans. Graph. (SIGGRAPH Asia)</i>
                    <br/>[ 
                    <a target="_blank" href="EigenPhysics.html">Project</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
                    <a href="EigenPhysics_files/preprint.pdf">Paper</a>&nbsp;
                    <a href="EigenPhysics_files/preprint.pdf"><img src="Research_files/Preprint.png" width="25" height="25" alt="Preprint" style="vertical-align:top"/></a> 
                    <a href="https://dl.acm.org/doi/10.1145/1457515.1409065?cid=81335499985"><img src="Research_files/dlauthorizer.jpg" width="25" height="25" alt="ACM DL Author-ize service" style="vertical-align:top"/></a> &nbsp; &nbsp; 
                    <a href="EigenPhysics_files/SIGA08.bib">BibTeX</a>  &nbsp; &nbsp; 
                    <a href="EigenPhysics_files/full.divx">Video</a>
                    <a href="EigenPhysics_files/full.divx"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a> &nbsp;
                    <a href="http://youtu.be/Ok6CDKQpQgE"><img src="Research_files/youtube.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]<br style="line-height: 2.0em"/>
                    We developed a novel algorithm to animate physically responsive virtual characters by combining kinematic pose control with dynamic constraints in the joint actuation space.<br/>
                </div>
                </p>
                               
                <p style="height: 80px;">
				<img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 136px; width: 176px; left: -10px; top: -20px; z-index: 1;"></img>
				<img src="Research_files/teaser_SIG07.jpg" style="position: relative; height: 120px; left: -182px; top: -29px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 136px; width: 176px; left: -354px; top: -20px;"></img>
				<div class="text">
                    <br/>Sumit Jain, Yuting Ye, C. Karen Liu. 2007. <b>Optimization-based Interactive Motion Synthesis for Virtual Characters.</b> <i>ACM SIGGRAPH Sketches and Posters</i>. (Third place in Student Research Competition) 
                    <br/>[ 
                    <a href="Research_files/SIG07_Sketch.pdf">Sketch</a>&nbsp;
                    <a href="Research_files/SIG07_Sketch.pdf"><img src="Research_files/Preprint.png" width="25" height="25" style="vertical-align:top"/></a>  
                    <a href="https://dl.acm.org/doi/10.1145/1278780.1278828"><img src="Research_files/acm.png" width="25" height="25" style="vertical-align:top"/></a>  &nbsp; 
                    <a href="Research_files/SIG07_Poster.jpg">Poster</a>&nbsp;
                    <a href="Research_files/SIG07_Poster.jpg"><img src="Research_files/Preprint.png" width="25" height="25" style="vertical-align:top"/></a>  
                    <a href="https://dl.acm.org/doi/10.1145/1280720.1280777"><img src="Research_files/acm.png" width="25" height="25" alt="ACM DL" style="vertical-align:top"/></a>  &nbsp;
                    <a href="Research_files/SIG07.bib">BibTeX</a> &nbsp; &nbsp;
                    <a href="https://www.dropbox.com/s/j2snvwy15f453py/SIG07_Sketch.mp4">Video</a>
                    <a href="https://www.dropbox.com/s/j2snvwy15f453py/SIG07_Sketch.mp4"><img src="Research_files/movie.png" width="25" height="25" style="vertical-align:top"/></a>
                    ]
                </div>
                </p>
            </div>

            <div class="item">
                <h1>Theses</h1>
            
                <p style="height: 120px;">
				<a href=ye_thesis.pdf>
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 138px; width: 177px; left: -10px; top: 10px; z-index: 1;"></img>
				<img src="Research_files/gatech.jpg" style="position: relative; height: 120px; left: -182px; top: -0px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 138px; width: 177px; left: -355px; top: 10px;"></img>
				</a>
                <div class="tighttext">
                    <br/>
                    <a href=ye_thesis.pdf><b>Simulation of characters with natural interactions</b></a>
                    <br/><i>Phd thesis</i>
                    <br/><i>Georgia Institute of Technology, 2012</i>
				</div>
                </p>
                
                <p style="height: 120px;">
                <img src="Research_files/imageEffectsAbove.png" style="position: relative; height: 123px; width: 177px; left: -10px; top: 10px; z-index: 1;"></img>
				<img src="OtherProjects_files/capture.jpg" style="position: relative; width: 160px; left: -182px; top: -0px; z-index: 1;"></img>
				<img src="Research_files/imageEffectsBelow.png" style="position: relative; height: 123px; width: 177px; left: -355px; top: 10px;"></img>
                <div class="tighttext">
                    <b>A momentum-based Bipedal Balance Controller</b>
                    <br/><i>Master's project</i>
                    <br/><i>University of Virginia, 2006</i>
                    <br/><a href="OtherProjects_files/A Momentum-based Bipedal Balance Controller.ppt">Presentation</a> (ppt, 1.09M)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="OtherProjects_files/video.zip">Video</a> (zip, 3.4M)
                </div>
				</p>

                <p style="height: 120px;">
				<img src="OtherProjects_files/pic1.jpg" style="position: relative; height: 138px; left: 15px; top: -0px; z-index: 1;"></img>
				<div class="tighttext">
                    <b>An Interactive 2D Vector Graphics Editing System</b>
				    <br/><i>Undergraduate thesis</i>
                    <br/><i>Peking University, 2004 </i>
				</div>
				</p>


            </div>
            
            <div class="item">
                <h1><a target="_blank" href="OtherProjects.html">Class projects</a></h1>
            </div>

        </div>
        
        <div class="footer">
        
            <span class="left">Copyright &copy; 2007 <a href="mailto:yutingye.public@gmail.com">Yuting Ye</a></span>
        
            <span class="right"><a href="http://templates.arcsin.se/">Website template</a> by <a href="http://arcsin.se/">Arcsin</a></span>

            <div class="clearer"><span></span></div>
        
        </div>

    </div>  

</div>

</body>

</html>
